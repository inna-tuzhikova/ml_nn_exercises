{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import ImageOps, ImageEnhance, ImageFilter, Image\n",
    "from tqdm import tqdm_notebook\n",
    "from functools import partial\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 20, 20\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple wrappers for functional-like model building\n",
    "\n",
    "class Flatten:\n",
    "    def __call__(self, input_tensor):\n",
    "        return tf.reshape(input_tensor, (tf.shape(input_tensor)[0], -1))\n",
    "    \n",
    "class Dense:\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        unif_init_range = 1.0 / (out_planes)**(0.5)\n",
    "        initializer = tf.random_uniform_initializer(-unif_init_range, unif_init_range)\n",
    "        self.weights = tf.get_variable('weights', shape=[in_planes, out_planes], initializer=initializer,\n",
    "                                       regularizer=tf.contrib.layers.l2_regularizer(scale=5e-5))\n",
    "        self.biases = tf.get_variable('biases', shape=[out_planes], initializer=tf.constant_initializer(0.0),\n",
    "                                      regularizer=tf.contrib.layers.l2_regularizer(scale=5e-5))\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        return tf.nn.xw_plus_b(input_tensor, self.weights, self.biases) \n",
    "    \n",
    "class Conv2d:\n",
    "    def __init__(self, in_planes, out_planes, filters: tuple, strides=(1, 1), padding='SAME', *args, **kwargs):\n",
    "        self.strides = (1, *strides, 1)\n",
    "        self.padding = padding.upper()\n",
    "        self.out_planes = out_planes\n",
    "        n = int(filters[0] * filters[1] * out_planes)\n",
    "        initializer = tf.random_normal_initializer(stddev=np.sqrt(2.0 / n))\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        self.kernels = tf.get_variable(\"kernels\", shape=[filters[0], filters[1], in_planes, out_planes], initializer=initializer,\n",
    "                                       regularizer=tf.contrib.layers.l2_regularizer(scale=5e-5))\n",
    "        \n",
    "    def __call__(self, input_tensor):\n",
    "        return tf.nn.conv2d(input_tensor, filter=self.kernels, strides=self.strides, padding=self.padding, *self.args, *self.kwargs)\n",
    "    \n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "    \n",
    "class AvgPool:\n",
    "    def __init__(self, kernel, strides=(1, 1), padding='VALID'):\n",
    "        self.kernel = (1, *kernel, 1)\n",
    "        self.strides = (1, *strides, 1)\n",
    "        self.padding = padding.upper()\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return tf.nn.avg_pool(x, ksize=self.kernel, strides=self.strides, padding=self.padding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck:\n",
    "    outchannel_ratio = 4\n",
    "\n",
    "    def __init__(self, inplanes, outplanes, alpha=(-1, 1), beta=(0, 1), prob=None, \n",
    "                 is_training=True, phase=None, strides=(1, 1), downsample=None):\n",
    "        assert alpha[1] > alpha[0] and beta[1] > beta[0] \n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.is_training = is_training\n",
    "        self.prob = prob\n",
    "        with tf.variable_scope(\"bn1\"):\n",
    "            self.bn1 = partial(tf.layers.batch_normalization, momentum=0.9, epsilon=1e-5, center=True, scale=True, \n",
    "                               training=phase, fused=True)\n",
    "        with tf.variable_scope(\"conv1\"):\n",
    "            self.conv1 = Conv2d(inplanes, outplanes, (1, 1))\n",
    "        with tf.variable_scope(\"bn2\"):\n",
    "            self.bn2 = partial(tf.layers.batch_normalization, momentum=0.9, epsilon=1e-5, center=True, scale=True, \n",
    "                               training=phase, fused=True)\n",
    "        with tf.variable_scope(\"conv2\"):\n",
    "            self.conv2 = Conv2d(outplanes, (outplanes * 1), (3, 3), strides=strides)\n",
    "        with tf.variable_scope(\"bn3\"):\n",
    "            self.bn3 = partial(tf.layers.batch_normalization, momentum=0.9, epsilon=1e-5, center=True, scale=True, \n",
    "                               training=phase, fused=True)\n",
    "        with tf.variable_scope(\"conv3\"):\n",
    "            self.conv3 = Conv2d((outplanes * 1), outplanes * Bottleneck.outchannel_ratio, (1, 1))\n",
    "        with tf.variable_scope(\"bn4\"):\n",
    "            self.bn4 = partial(tf.layers.batch_normalization, momentum=0.9, epsilon=1e-5, center=True, scale=True, \n",
    "                               training=phase, fused=True)\n",
    "        self.relu = tf.nn.relu\n",
    "        self.downsample = downsample\n",
    "        self.strides = strides\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    " \n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        out = self.bn4(out)\n",
    "            \n",
    "        batch_size = tf.shape(out)[0]\n",
    "            \n",
    "        # Apply shake-drop regularization\n",
    "        # Got from https://openreview.net/pdf?id=S1NHaMW0b\n",
    "        if self.prob is not None:\n",
    "            if self.is_training:\n",
    "                bern_shape = [batch_size, 1, 1, 1]\n",
    "                random_tensor = self.prob\n",
    "                random_tensor += tf.random_uniform(bern_shape, dtype=tf.float32)\n",
    "                binary_tensor = tf.floor(random_tensor)\n",
    "\n",
    "                alpha_values = tf.random_uniform(\n",
    "                    [batch_size, 1, 1, 1], minval=self.alpha[0], maxval=self.alpha[1],\n",
    "                    dtype=tf.float32)\n",
    "                beta_values = tf.random_uniform(\n",
    "                    [batch_size, 1, 1, 1], minval=self.beta[0], maxval=self.beta[1],\n",
    "                    dtype=tf.float32)\n",
    "\n",
    "                rand_forward = (binary_tensor + alpha_values - binary_tensor * alpha_values)\n",
    "                rand_backward = (binary_tensor + beta_values - binary_tensor * beta_values)\n",
    "                out = out * rand_backward + tf.stop_gradient(out * rand_forward - out * rand_backward)\n",
    "            else:\n",
    "                expected_alpha = (self.alpha[1] + self.alpha[0]) / 2\n",
    "                out = (self.prob + expected_alpha - self.prob * expected_alpha) * out\n",
    "            \n",
    "        if self.downsample is not None:\n",
    "            shortcut = self.downsample(x)\n",
    "            featuremap_size = tf.shape(shortcut)[1:3]\n",
    "        else:\n",
    "            shortcut = x\n",
    "            featuremap_size = tf.shape(out)[1:3]\n",
    "        \n",
    "        residual_channel = tf.shape(out)[3]\n",
    "        shortcut_channel = tf.shape(shortcut)[3]\n",
    "        \n",
    "        padding = tf.zeros((batch_size, featuremap_size[0], featuremap_size[1], residual_channel - shortcut_channel))\n",
    "        return out + tf.concat([shortcut, padding], axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got from https://arxiv.org/pdf/1610.02915.pdf\n",
    "\n",
    "class PyramidNet:\n",
    "    def __init__(self, depth, alpha, num_classes, is_training=True, phase=None, shake_drop=False, reuse=False):\n",
    "        self.inplanes = 16\n",
    "        n = int((depth - 2) / 9)\n",
    "        self.total_layers = n * 3\n",
    "        \n",
    "        # Best params due the shake-drop paper\n",
    "        self.shake_drop = shake_drop\n",
    "        self.alpha_shake = (-1, 1)\n",
    "        self.beta_shake = (0, 1)\n",
    "        self.p_l = 0.5\n",
    "        self.current_layer = 0\n",
    "        self.phase = phase\n",
    "\n",
    "        self.is_training = is_training\n",
    "        self.addrate = alpha / (3 * n * 1.0)\n",
    "\n",
    "        self.input_featuremap_dim = self.inplanes\n",
    "        with tf.variable_scope(\"pyramidnet_conv1\"):\n",
    "            self.conv1 = Conv2d(3, self.input_featuremap_dim, filters=(3, 3), strides=(1, 1))\n",
    "        with tf.variable_scope(\"pyramidnet_bn1\"):\n",
    "            self.bn1 = partial(tf.layers.batch_normalization, momentum=0.9, epsilon=1e-5, center=True, scale=True, \n",
    "                               training=self.phase, fused=True)\n",
    "\n",
    "        self.featuremap_dim = self.input_featuremap_dim \n",
    "        with tf.variable_scope(\"pyramidnet_layer1\"):\n",
    "            self.layer1 = self.pyramidal_make_layer(n)\n",
    "        with tf.variable_scope(\"pyramidnet_layer2\"):\n",
    "            self.layer2 = self.pyramidal_make_layer(n, strides=(2, 2))\n",
    "        with tf.variable_scope(\"pyramidnet_layer3\"):\n",
    "            self.layer3 = self.pyramidal_make_layer(n, strides=(2, 2))\n",
    "\n",
    "        self.final_featuremap_dim = self.input_featuremap_dim\n",
    "        with tf.variable_scope(\"pyramidnet_bn_final\"):\n",
    "            self.bn_final = partial(tf.layers.batch_normalization, momentum=0.9, epsilon=1e-5, center=True, scale=True, \n",
    "                               training=self.phase, fused=True)\n",
    "        self.relu_final = tf.nn.relu\n",
    "        self.avgpool = AvgPool((8, 8), strides=(8, 8))  # TODO: Think about SSP\n",
    "        self.flatten = Flatten()\n",
    "        with tf.variable_scope(\"pyramidnet_fc\"):\n",
    "            self.fc = Dense(self.final_featuremap_dim, num_classes)\n",
    "\n",
    "    def pyramidal_make_layer(self, block_depth, strides=(1, 1)):\n",
    "        downsample = None\n",
    "        if strides != (1, 1):\n",
    "            downsample = AvgPool((2, 2), strides=(2, 2))\n",
    "\n",
    "        layers = []\n",
    "        self.featuremap_dim = self.featuremap_dim + self.addrate\n",
    "        prob = self.calc_prob()\n",
    "        with tf.variable_scope(\"bottleneck_1\"):\n",
    "            layers.append(Bottleneck(self.input_featuremap_dim, int(round(self.featuremap_dim)), \n",
    "                                     alpha=self.alpha_shake, beta=self.beta_shake, prob=prob, # shake-drop regularization here\n",
    "                                     is_training=self.is_training, phase=self.phase, strides=strides, downsample=downsample))\n",
    "        for i in range(1, block_depth):\n",
    "            temp_featuremap_dim = self.featuremap_dim + self.addrate\n",
    "            prob = self.calc_prob()\n",
    "            with tf.variable_scope(f\"bottleneck_{i+1}\"):\n",
    "                layers.append(Bottleneck(int(round(self.featuremap_dim)) * Bottleneck.outchannel_ratio, int(round(temp_featuremap_dim)),\n",
    "                                         alpha=self.alpha_shake, beta=self.beta_shake, prob=prob, # shake-drop regularization here\n",
    "                                         is_training=self.is_training, phase=self.phase, strides=(1, 1)))\n",
    "            self.featuremap_dim  = temp_featuremap_dim\n",
    "        self.input_featuremap_dim = int(round(self.featuremap_dim)) * Bottleneck.outchannel_ratio\n",
    "\n",
    "        return Sequential(layers)\n",
    "    \n",
    "    def calc_prob(self):\n",
    "        if not self.shake_drop:\n",
    "            return None\n",
    "        self.current_layer += 1\n",
    "        return 1 - (float(self.current_layer) / self.total_layers) * self.p_l\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.conv1(x)\n",
    "        self.test_bn = self.bn1(x) \n",
    "        x = self.test_bn\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.bn_final(x)\n",
    "        x = self.relu_final(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HParams for cifar10 normalization\n",
    "\n",
    "# Got from https://arxiv.org/abs/1805.09501\n",
    "IMAGE_SIZE = 32\n",
    "MEANS = [0.49139968, 0.48215841, 0.44653091]\n",
    "STDS = [0.24703223, 0.24348513, 0.26158784]\n",
    "PARAMETER_MAX = 10\n",
    "\n",
    "def random_flip(x):\n",
    "    if np.random.rand(1)[0] > 0.5:\n",
    "        return np.fliplr(x)\n",
    "    return x\n",
    "\n",
    "def zero_pad_and_crop(img, amount=4):\n",
    "    padded_img = np.zeros((img.shape[0] + amount * 2, img.shape[1] + amount * 2,\n",
    "                           img.shape[2]))\n",
    "    padded_img[amount:img.shape[0] + amount, amount:img.shape[1] + amount, :] = img\n",
    "    top = np.random.randint(low=0, high=2 * amount)\n",
    "    left = np.random.randint(low=0, high=2 * amount)\n",
    "    new_img = padded_img[top:top + img.shape[0], left:left + img.shape[1], :]\n",
    "    return new_img\n",
    "\n",
    "def create_cutout_mask(img_height, img_width, num_channels, size):\n",
    "    assert img_height == img_width\n",
    "\n",
    "    height_loc = np.random.randint(low=0, high=img_height)\n",
    "    width_loc = np.random.randint(low=0, high=img_width)\n",
    "\n",
    "    upper_coord = (max(0, height_loc - size // 2), max(0, width_loc - size // 2))\n",
    "    lower_coord = (min(img_height, height_loc + size // 2),\n",
    "                   min(img_width, width_loc + size // 2))\n",
    "    mask_height = lower_coord[0] - upper_coord[0]\n",
    "    mask_width = lower_coord[1] - upper_coord[1]\n",
    "\n",
    "    mask = np.ones((img_height, img_width, num_channels))\n",
    "    zeros = np.zeros((mask_height, mask_width, num_channels))\n",
    "    mask[upper_coord[0]:lower_coord[0], upper_coord[1]:lower_coord[1], :] = (zeros)\n",
    "    return mask, upper_coord, lower_coord\n",
    "\n",
    "def cutout_numpy(img, size=16):\n",
    "    img_height, img_width, num_channels = (img.shape[0], img.shape[1], img.shape[2])\n",
    "    assert len(img.shape) == 3\n",
    "    mask, _, _ = create_cutout_mask(img_height, img_width, num_channels, size)\n",
    "    return img * mask\n",
    "\n",
    "def float_parameter(level, maxval):\n",
    "    return float(level) * maxval / PARAMETER_MAX\n",
    "\n",
    "def int_parameter(level, maxval):\n",
    "    return int(level * maxval / PARAMETER_MAX)\n",
    "\n",
    "def pil_wrap(img):\n",
    "    return Image.fromarray(np.uint8((img * STDS + MEANS) * 255.0)).convert('RGBA')\n",
    "\n",
    "def pil_unwrap(pil_img):\n",
    "    pic_array = (np.array(pil_img.getdata()).reshape((32, 32, 4)) / 255.0)\n",
    "    i1, i2 = np.where(pic_array[:, :, 3] == 0)\n",
    "    pic_array = (pic_array[:, :, :3] - MEANS) / STDS\n",
    "    pic_array[i1, i2] = [0, 0, 0]\n",
    "    return pic_array\n",
    "\n",
    "def apply_policy(policy, img):\n",
    "    pil_img = pil_wrap(img)\n",
    "    for xform in policy:\n",
    "        assert len(xform) == 3\n",
    "        name, probability, level = xform\n",
    "        xform_fn = NAME_TO_TRANSFORM[name].pil_transformer(probability, level)\n",
    "        pil_img = xform_fn(pil_img)\n",
    "    return pil_unwrap(pil_img)\n",
    "\n",
    "class TransformFunction(object):\n",
    "    def __init__(self, func, name):\n",
    "        self.f = func\n",
    "        self.name = name\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<' + self.name + '>'\n",
    "\n",
    "    def __call__(self, pil_img):\n",
    "        return self.f(pil_img)\n",
    "\n",
    "class TransformT(object):\n",
    "    def __init__(self, name, xform_fn):\n",
    "        self.name = name\n",
    "        self.xform = xform_fn\n",
    "\n",
    "    def pil_transformer(self, probability, level):\n",
    "        def return_function(im):\n",
    "            if random.random() < probability:\n",
    "                im = self.xform(im, level)\n",
    "            return im\n",
    "        name = self.name + '({:.1f},{})'.format(probability, level)\n",
    "        return TransformFunction(return_function, name)\n",
    "\n",
    "    def do_transform(self, image, level):\n",
    "        f = self.pil_transformer(PARAMETER_MAX, level)\n",
    "        return pil_unwrap(f(pil_wrap(image)))\n",
    "\n",
    "identity = TransformT('identity', lambda pil_img, level: pil_img)\n",
    "flip_lr = TransformT('FlipLR', lambda pil_img, level: pil_img.transpose(Image.FLIP_LEFT_RIGHT))\n",
    "flip_ud = TransformT('FlipUD', lambda pil_img, level: pil_img.transpose(Image.FLIP_TOP_BOTTOM))\n",
    "auto_contrast = TransformT('AutoContrast', lambda pil_img, level: ImageOps.autocontrast(pil_img.convert('RGB')).convert('RGBA'))\n",
    "equalize = TransformT('Equalize', lambda pil_img, level: ImageOps.equalize(pil_img.convert('RGB')).convert('RGBA'))\n",
    "invert = TransformT('Invert', lambda pil_img, level: ImageOps.invert(pil_img.convert('RGB')).convert('RGBA'))\n",
    "blur = TransformT('Blur', lambda pil_img, level: pil_img.filter(ImageFilter.BLUR))\n",
    "smooth = TransformT('Smooth', lambda pil_img, level: pil_img.filter(ImageFilter.SMOOTH))\n",
    "\n",
    "def _rotate_impl(pil_img, level):\n",
    "    degrees = int_parameter(level, 30)\n",
    "    if random.random() > 0.5:\n",
    "        degrees = -degrees\n",
    "    return pil_img.rotate(degrees)\n",
    "\n",
    "rotate = TransformT('Rotate', _rotate_impl)\n",
    "\n",
    "def _posterize_impl(pil_img, level):\n",
    "    level = int_parameter(level, 4)\n",
    "    return ImageOps.posterize(pil_img.convert('RGB'), 4 - level).convert('RGBA')\n",
    "\n",
    "posterize = TransformT('Posterize', _posterize_impl)\n",
    "\n",
    "def _shear_x_impl(pil_img, level):\n",
    "    level = float_parameter(level, 0.3)\n",
    "    if random.random() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((32, 32), Image.AFFINE, (1, level, 0, 0, 1, 0))\n",
    "\n",
    "shear_x = TransformT('ShearX', _shear_x_impl)\n",
    "\n",
    "def _shear_y_impl(pil_img, level):\n",
    "    level = float_parameter(level, 0.3)\n",
    "    if random.random() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((32, 32), Image.AFFINE, (1, 0, 0, level, 1, 0))\n",
    "\n",
    "shear_y = TransformT('ShearY', _shear_y_impl)\n",
    "\n",
    "def _translate_x_impl(pil_img, level):\n",
    "    level = int_parameter(level, 10)\n",
    "    if random.random() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((32, 32), Image.AFFINE, (1, 0, level, 0, 1, 0))\n",
    "\n",
    "translate_x = TransformT('TranslateX', _translate_x_impl)\n",
    "\n",
    "def _translate_y_impl(pil_img, level):\n",
    "    level = int_parameter(level, 10)\n",
    "    if random.random() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((32, 32), Image.AFFINE, (1, 0, 0, 0, 1, level))\n",
    "\n",
    "translate_y = TransformT('TranslateY', _translate_y_impl)\n",
    "\n",
    "def _crop_impl(pil_img, level, interpolation=Image.BILINEAR):\n",
    "    cropped = pil_img.crop((level, level, IMAGE_SIZE - level, IMAGE_SIZE - level))\n",
    "    resized = cropped.resize((IMAGE_SIZE, IMAGE_SIZE), interpolation)\n",
    "    return resized\n",
    "\n",
    "crop_bilinear = TransformT('CropBilinear', _crop_impl)\n",
    "\n",
    "def _solarize_impl(pil_img, level):\n",
    "    level = int_parameter(level, 256)\n",
    "    return ImageOps.solarize(pil_img.convert('RGB'), 256 - level).convert('RGBA')\n",
    "\n",
    "solarize = TransformT('Solarize', _solarize_impl)\n",
    "\n",
    "def _cutout_pil_impl(pil_img, level):\n",
    "    size = int_parameter(level, 20)\n",
    "    if size <= 0:\n",
    "        return pil_img\n",
    "    img_height, img_width, num_channels = (32, 32, 3)\n",
    "    _, upper_coord, lower_coord = (create_cutout_mask(img_height, img_width, num_channels, size))\n",
    "    pixels = pil_img.load()  # create the pixel map\n",
    "    for i in range(upper_coord[0], lower_coord[0]):  # for every col:\n",
    "        for j in range(upper_coord[1], lower_coord[1]):  # For every row\n",
    "            pixels[i, j] = (125, 122, 113, 0)  # set the colour accordingly\n",
    "    return pil_img\n",
    "\n",
    "cutout = TransformT('Cutout', _cutout_pil_impl)\n",
    "\n",
    "def _enhancer_impl(enhancer):\n",
    "    def impl(pil_img, level):\n",
    "        v = float_parameter(level, 1.8) + .1  # going to 0 just destroys it\n",
    "        return enhancer(pil_img).enhance(v)\n",
    "    return impl\n",
    "\n",
    "color = TransformT('Color', _enhancer_impl(ImageEnhance.Color))\n",
    "contrast = TransformT('Contrast', _enhancer_impl(ImageEnhance.Contrast))\n",
    "brightness = TransformT('Brightness', _enhancer_impl(ImageEnhance.Brightness))\n",
    "sharpness = TransformT('Sharpness', _enhancer_impl(ImageEnhance.Sharpness))\n",
    "\n",
    "ALL_TRANSFORMS = [flip_lr, flip_ud, auto_contrast, equalize, invert, rotate,\n",
    "    posterize, crop_bilinear, solarize, color, contrast, brightness, sharpness,\n",
    "    shear_x, shear_y, translate_x, translate_y, cutout, blur, smooth]\n",
    "NAME_TO_TRANSFORM = {t.name: t for t in ALL_TRANSFORMS}\n",
    "TRANSFORM_NAMES = NAME_TO_TRANSFORM.keys()\n",
    "\n",
    "def good_policies():\n",
    "    \"\"\"Format: (name, probability, level) \"\"\"\n",
    "    exp0_0 = [\n",
    "        [('Invert', 0.1, 7), ('Contrast', 0.2, 6)],\n",
    "        [('Rotate', 0.7, 2), ('TranslateX', 0.3, 9)],\n",
    "        [('Sharpness', 0.8, 1), ('Sharpness', 0.9, 3)],\n",
    "        [('ShearY', 0.5, 8), ('TranslateY', 0.7, 9)],\n",
    "        [('AutoContrast', 0.5, 8), ('Equalize', 0.9, 2)]]\n",
    "    exp0_1 = [\n",
    "        [('Solarize', 0.4, 5), ('AutoContrast', 0.9, 3)],\n",
    "        [('TranslateY', 0.9, 9), ('TranslateY', 0.7, 9)],\n",
    "        [('AutoContrast', 0.9, 2), ('Solarize', 0.8, 3)],\n",
    "        [('Equalize', 0.8, 8), ('Invert', 0.1, 3)],\n",
    "        [('TranslateY', 0.7, 9), ('AutoContrast', 0.9, 1)]]\n",
    "    exp0_2 = [\n",
    "        [('Solarize', 0.4, 5), ('AutoContrast', 0.0, 2)],\n",
    "        [('TranslateY', 0.7, 9), ('TranslateY', 0.7, 9)],\n",
    "        [('AutoContrast', 0.9, 0), ('Solarize', 0.4, 3)],\n",
    "        [('Equalize', 0.7, 5), ('Invert', 0.1, 3)],\n",
    "        [('TranslateY', 0.7, 9), ('TranslateY', 0.7, 9)]]\n",
    "    exp0_3 = [\n",
    "        [('Solarize', 0.4, 5), ('AutoContrast', 0.9, 1)],\n",
    "        [('TranslateY', 0.8, 9), ('TranslateY', 0.9, 9)],\n",
    "        [('AutoContrast', 0.8, 0), ('TranslateY', 0.7, 9)],\n",
    "        [('TranslateY', 0.2, 7), ('Color', 0.9, 6)],\n",
    "        [('Equalize', 0.7, 6), ('Color', 0.4, 9)]]\n",
    "    exp1_0 = [\n",
    "        [('ShearY', 0.2, 7), ('Posterize', 0.3, 7)],\n",
    "        [('Color', 0.4, 3), ('Brightness', 0.6, 7)],\n",
    "        [('Sharpness', 0.3, 9), ('Brightness', 0.7, 9)],\n",
    "        [('Equalize', 0.6, 5), ('Equalize', 0.5, 1)],\n",
    "        [('Contrast', 0.6, 7), ('Sharpness', 0.6, 5)]]\n",
    "    exp1_1 = [\n",
    "        [('Brightness', 0.3, 7), ('AutoContrast', 0.5, 8)],\n",
    "        [('AutoContrast', 0.9, 4), ('AutoContrast', 0.5, 6)],\n",
    "        [('Solarize', 0.3, 5), ('Equalize', 0.6, 5)],\n",
    "        [('TranslateY', 0.2, 4), ('Sharpness', 0.3, 3)],\n",
    "        [('Brightness', 0.0, 8), ('Color', 0.8, 8)]]\n",
    "    exp1_2 = [\n",
    "        [('Solarize', 0.2, 6), ('Color', 0.8, 6)],\n",
    "        [('Solarize', 0.2, 6), ('AutoContrast', 0.8, 1)],\n",
    "        [('Solarize', 0.4, 1), ('Equalize', 0.6, 5)],\n",
    "        [('Brightness', 0.0, 0), ('Solarize', 0.5, 2)],\n",
    "        [('AutoContrast', 0.9, 5), ('Brightness', 0.5, 3)]]\n",
    "    exp1_3 = [\n",
    "        [('Contrast', 0.7, 5), ('Brightness', 0.0, 2)],\n",
    "        [('Solarize', 0.2, 8), ('Solarize', 0.1, 5)],\n",
    "        [('Contrast', 0.5, 1), ('TranslateY', 0.2, 9)],\n",
    "        [('AutoContrast', 0.6, 5), ('TranslateY', 0.0, 9)],\n",
    "        [('AutoContrast', 0.9, 4), ('Equalize', 0.8, 4)]]\n",
    "    exp1_4 = [\n",
    "        [('Brightness', 0.0, 7), ('Equalize', 0.4, 7)],\n",
    "        [('Solarize', 0.2, 5), ('Equalize', 0.7, 5)],\n",
    "        [('Equalize', 0.6, 8), ('Color', 0.6, 2)],\n",
    "        [('Color', 0.3, 7), ('Color', 0.2, 4)],\n",
    "        [('AutoContrast', 0.5, 2), ('Solarize', 0.7, 2)]]\n",
    "    exp1_5 = [\n",
    "        [('AutoContrast', 0.2, 0), ('Equalize', 0.1, 0)],\n",
    "        [('ShearY', 0.6, 5), ('Equalize', 0.6, 5)],\n",
    "        [('Brightness', 0.9, 3), ('AutoContrast', 0.4, 1)],\n",
    "        [('Equalize', 0.8, 8), ('Equalize', 0.7, 7)],\n",
    "        [('Equalize', 0.7, 7), ('Solarize', 0.5, 0)]]\n",
    "    exp1_6 = [\n",
    "        [('Equalize', 0.8, 4), ('TranslateY', 0.8, 9)],\n",
    "        [('TranslateY', 0.8, 9), ('TranslateY', 0.6, 9)],\n",
    "        [('TranslateY', 0.9, 0), ('TranslateY', 0.5, 9)],\n",
    "        [('AutoContrast', 0.5, 3), ('Solarize', 0.3, 4)],\n",
    "        [('Solarize', 0.5, 3), ('Equalize', 0.4, 4)]]\n",
    "    exp2_0 = [\n",
    "        [('Color', 0.7, 7), ('TranslateX', 0.5, 8)],\n",
    "        [('Equalize', 0.3, 7), ('AutoContrast', 0.4, 8)],\n",
    "        [('TranslateY', 0.4, 3), ('Sharpness', 0.2, 6)],\n",
    "        [('Brightness', 0.9, 6), ('Color', 0.2, 8)],\n",
    "        [('Solarize', 0.5, 2), ('Invert', 0.0, 3)]]\n",
    "    exp2_1 = [\n",
    "        [('AutoContrast', 0.1, 5), ('Brightness', 0.0, 0)],\n",
    "        [('Cutout', 0.2, 4), ('Equalize', 0.1, 1)],\n",
    "        [('Equalize', 0.7, 7), ('AutoContrast', 0.6, 4)],\n",
    "        [('Color', 0.1, 8), ('ShearY', 0.2, 3)],\n",
    "        [('ShearY', 0.4, 2), ('Rotate', 0.7, 0)]]\n",
    "    exp2_2 = [\n",
    "        [('ShearY', 0.1, 3), ('AutoContrast', 0.9, 5)],\n",
    "        [('TranslateY', 0.3, 6), ('Cutout', 0.3, 3)],\n",
    "        [('Equalize', 0.5, 0), ('Solarize', 0.6, 6)],\n",
    "        [('AutoContrast', 0.3, 5), ('Rotate', 0.2, 7)],\n",
    "        [('Equalize', 0.8, 2), ('Invert', 0.4, 0)]]\n",
    "    exp2_3 = [\n",
    "        [('Equalize', 0.9, 5), ('Color', 0.7, 0)],\n",
    "        [('Equalize', 0.1, 1), ('ShearY', 0.1, 3)],\n",
    "        [('AutoContrast', 0.7, 3), ('Equalize', 0.7, 0)],\n",
    "        [('Brightness', 0.5, 1), ('Contrast', 0.1, 7)],\n",
    "        [('Contrast', 0.1, 4), ('Solarize', 0.6, 5)]]\n",
    "    exp2_4 = [\n",
    "        [('Solarize', 0.2, 3), ('ShearX', 0.0, 0)],\n",
    "        [('TranslateX', 0.3, 0), ('TranslateX', 0.6, 0)],\n",
    "        [('Equalize', 0.5, 9), ('TranslateY', 0.6, 7)],\n",
    "        [('ShearX', 0.1, 0), ('Sharpness', 0.5, 1)],\n",
    "        [('Equalize', 0.8, 6), ('Invert', 0.3, 6)]]\n",
    "    exp2_5 = [\n",
    "        [('AutoContrast', 0.3, 9), ('Cutout', 0.5, 3)],\n",
    "        [('ShearX', 0.4, 4), ('AutoContrast', 0.9, 2)],\n",
    "        [('ShearX', 0.0, 3), ('Posterize', 0.0, 3)],\n",
    "        [('Solarize', 0.4, 3), ('Color', 0.2, 4)],\n",
    "        [('Equalize', 0.1, 4), ('Equalize', 0.7, 6)]]\n",
    "    exp2_6 = [\n",
    "        [('Equalize', 0.3, 8), ('AutoContrast', 0.4, 3)],\n",
    "        [('Solarize', 0.6, 4), ('AutoContrast', 0.7, 6)],\n",
    "        [('AutoContrast', 0.2, 9), ('Brightness', 0.4, 8)],\n",
    "        [('Equalize', 0.1, 0), ('Equalize', 0.0, 6)],\n",
    "        [('Equalize', 0.8, 4), ('Equalize', 0.0, 4)]]\n",
    "    exp2_7 = [\n",
    "        [('Equalize', 0.5, 5), ('AutoContrast', 0.1, 2)],\n",
    "        [('Solarize', 0.5, 5), ('AutoContrast', 0.9, 5)],\n",
    "        [('AutoContrast', 0.6, 1), ('AutoContrast', 0.7, 8)],\n",
    "        [('Equalize', 0.2, 0), ('AutoContrast', 0.1, 2)],\n",
    "        [('Equalize', 0.6, 9), ('Equalize', 0.4, 4)]]\n",
    "    exp0s = exp0_0 + exp0_1 + exp0_2 + exp0_3\n",
    "    exp1s = exp1_0 + exp1_1 + exp1_2 + exp1_3 + exp1_4 + exp1_5 + exp1_6\n",
    "    exp2s = exp2_0 + exp2_1 + exp2_2 + exp2_3 + exp2_4 + exp2_5 + exp2_6 + exp2_7\n",
    "    return  exp0s + exp1s + exp2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common helpers\n",
    "\n",
    "def preprocess_data(datasets):\n",
    "    out = []\n",
    "    for data in datasets:\n",
    "        data = data.reshape(-1, 32, 32, 3)\n",
    "        data = data / 255.0\n",
    "        out.append((data - MEANS) / STDS)\n",
    "    return out\n",
    "\n",
    "def aug_batch(policies, batch):\n",
    "    epoch_policy = policies[np.random.choice(len(policies))]\n",
    "    out = np.zeros_like(batch).astype(np.float32)\n",
    "    for i, image in enumerate(batch):\n",
    "        image = apply_policy(epoch_policy, image)\n",
    "        image = random_flip(zero_pad_and_crop(image, 4))\n",
    "        image = cutout_numpy(image)\n",
    "        out[i] = image\n",
    "    return out\n",
    "\n",
    "def cosine_lr(learning_rate, epoch, iteration, batches_per_epoch, total_epochs=1800):\n",
    "    t_total = total_epochs * batches_per_epoch\n",
    "    t_cur = float(epoch * batches_per_epoch + iteration)\n",
    "    return 0.5 * learning_rate * (1 + np.cos(np.pi * t_cur / t_total))\n",
    "\n",
    "def get_lr(curr_epoch, iteration=None, initial_lr=0.05, batch_size=128, num_epochs=800):\n",
    "    assert iteration is not None\n",
    "    batches_per_epoch = int(5000 / batch_size)\n",
    "    lr = cosine_lr(initial_lr, curr_epoch, iteration, batches_per_epoch, num_epochs)\n",
    "    return lr\n",
    "\n",
    "def save_results(sess, saver, batch_size=32):\n",
    "    test_results = []\n",
    "    accuracy_results = []\n",
    "\n",
    "    index_test = np.arange(len(x_test))\n",
    "    num_batches_test = int(len(index_test) / batch_size)\n",
    "    batch_indexes_test = np.array_split(index_test, num_batches_test)\n",
    "\n",
    "    for batch_index in tqdm_notebook(batch_indexes_test, leave=False):\n",
    "        feed_dict = {x: x_test[batch_index], y: y_test[batch_index], phase: 0, learning_rate:0.0}\n",
    "        out, = sess.run([logits], feed_dict=feed_dict)\n",
    "        for i, batch_i in enumerate(batch_index):\n",
    "            y_pred = np.argmax(out[i])\n",
    "            test_results.append([batch_i, y_pred])\n",
    "            accuracy_results.append([y_pred, y_test[batch_i]])\n",
    "\n",
    "    accuracy_results = np.array(accuracy_results)\n",
    "    test_results = np.array(test_results).astype(np.int)\n",
    "    correct_prediction = np.equal(accuracy_results[:, 0], accuracy_results[:, 1])\n",
    "    np.savetxt(\"results.csv\", test_results, delimiter=\",\", fmt='%d')\n",
    "    print(f\"Saved with accuracy: {np.mean(correct_prediction.astype(np.float32)):.6}\")\n",
    "    \n",
    "def restore():\n",
    "    with tf.device('/cpu:0'):\n",
    "        saver = tf.train.Saver()\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        sess = tf.Session()\n",
    "        saver.restore(sess, \"./model.ckpt\")\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train, y_test = np.squeeze(y_train), np.squeeze(y_test)\n",
    "x_train, x_test = preprocess_data([x_train, x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "need_to_restore_last = True\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "phase = tf.placeholder(tf.bool, name='phase')  # 'is_training' for shake-drop regularization to prevent extra tf.cond, 'phase' is for BN \n",
    "\n",
    "model_args = {'depth': 272, 'alpha': 200, 'num_classes': 10, 'shake_drop': True}\n",
    "\n",
    "with tf.variable_scope('model', use_resource=False) as scope:\n",
    "    model = PyramidNet(is_training=True, phase=phase, **model_args)\n",
    "    scope.reuse_variables()\n",
    "    eval_model = PyramidNet(is_training=False, phase=phase, **model_args)\n",
    "    \n",
    "logits = model(x)\n",
    "total_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "mean_loss += tf.losses.get_regularization_loss()\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "eval_logits = eval_model(x)\n",
    "eval_correct_prediction = tf.equal(tf.argmax(eval_logits, 1), y)\n",
    "eval_accuracy = tf.reduce_mean(tf.cast(eval_correct_prediction, tf.float32))\n",
    "\n",
    "train_summaries = tf.summary.merge([tf.summary.scalar('loss', mean_loss), tf.summary.scalar('accuracy', accuracy)])\n",
    "eval_summaries = tf.summary.merge([tf.summary.scalar('eval_accuracy', eval_accuracy)])\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n",
    "    optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, clip_norm=5.0)\n",
    "    train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "if not need_to_restore_last:\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "        sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\n",
    "else:\n",
    "    sess = restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 1400\n",
    "epochs = 1800\n",
    "batch_size = 16\n",
    "current_lr = 0.05\n",
    "\n",
    "index = np.arange(len(x_train))\n",
    "num_batches = int(len(index) / batch_size)\n",
    "\n",
    "index_test = np.arange(len(x_test))\n",
    "num_batches_test = int(len(index_test) / batch_size)\n",
    "batch_indexes_test = np.array_split(index_test, num_batches_test)\n",
    "\n",
    "policies = good_policies()\n",
    "iteration = (len(x_train) * initial_epoch) // batch_size\n",
    "shutil.rmtree(\"./logs/\")\n",
    "summary_writer = tf.summary.FileWriter(f\"./logs/pyramidnet_{model_args['depth']}/\", graph=sess.graph)\n",
    "    \n",
    "for e in tqdm_notebook(range(initial_epoch, epochs), desc='Epochs:'):\n",
    "    \n",
    "    # set warmup stages without augment and shuffle data every epoch\n",
    "    augment = True\n",
    "    np.random.shuffle(index)\n",
    "    batch_indexes = np.array_split(index, num_batches)\n",
    "\n",
    "    # train epoch\n",
    "    batch_gen = tqdm_notebook(enumerate(batch_indexes), leave=False)\n",
    "    losses = []\n",
    "    for i, batch_index in batch_gen:\n",
    "        if augment:\n",
    "            current_lr = get_lr(e, i + 1, num_epochs=epochs, batch_size=batch_size)\n",
    "            x_batch = aug_batch(policies, x_train[batch_index])\n",
    "            feed_dict = {x: x_batch, y: y_train[batch_index], phase: 1, learning_rate: current_lr}\n",
    "        else:\n",
    "            feed_dict = {x: x_train[batch_index], y: y_train[batch_index], phase: 1, learning_rate: current_lr}\n",
    "\n",
    "        scores, loss, acc, _, summ_data = sess.run([logits, mean_loss, accuracy, train_step, train_summaries], feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summ_data, iteration)\n",
    "        iteration += 1\n",
    "        batch_gen.set_postfix(loss=loss, acc=acc, lr=current_lr)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    # validation step\n",
    "    val_accuracy = []\n",
    "    for i, batch_index in tqdm_notebook(enumerate(batch_indexes_test), leave=False):\n",
    "        feed_dict = {x: x_test[batch_index], y: y_test[batch_index], phase: 0, learning_rate:0.0}\n",
    "        acc, summ_data = sess.run([eval_accuracy, eval_summaries], feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summ_data, (e * int(len(index_test) / batch_size)) + i)\n",
    "        val_accuracy.append(acc)\n",
    "    \n",
    "    print(f'train loss: {np.mean(losses):.6}, eval accuracy: {np.mean(val_accuracy):.6}')\n",
    "    \n",
    "    if e % 50 == 0 and e > 0:\n",
    "        save_results(sess, saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved with accuracy: 0.9751\n"
     ]
    }
   ],
   "source": [
    "test_results = []\n",
    "accuracy_results = []\n",
    "\n",
    "index_test = np.arange(len(x_test))\n",
    "num_batches_test = int(len(index_test) / batch_size)\n",
    "batch_indexes_test = np.array_split(index_test, num_batches_test)\n",
    "\n",
    "for batch_index in tqdm_notebook(batch_indexes_test, leave=False):\n",
    "    feed_dict = {x: x_test[batch_index], y: y_test[batch_index], phase: 0, learning_rate:0.0}\n",
    "    out, = sess.run([logits], feed_dict=feed_dict)\n",
    "    for i, batch_i in enumerate(batch_index):\n",
    "        y_pred = np.argmax(out[i])\n",
    "        test_results.append([batch_i, y_pred])\n",
    "        accuracy_results.append([y_pred, y_test[batch_i]])\n",
    "\n",
    "accuracy_results = np.array(accuracy_results)\n",
    "test_results = np.array(test_results).astype(np.int)\n",
    "correct_prediction = np.equal(accuracy_results[:, 0], accuracy_results[:, 1])\n",
    "np.savetxt(\"results.csv\", test_results, delimiter=\",\", fmt='%d')\n",
    "print(f\"Saved with accuracy: {np.mean(correct_prediction.astype(np.float32)):.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
